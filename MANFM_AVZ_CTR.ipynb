{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKqiwhWZJ0Qf",
        "outputId": "35f9fbaf-b142-4802-bbcd-15aba0525900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepCTR' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shenweichen/DeepCTR.git\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/DeepCTR')"
      ],
      "metadata": {
        "id": "7w5EdCZqJ4R_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIVEdYiaJ5--",
        "outputId": "fe7272cc-9121-4db2-c23a-900364d4fdfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/data/train.csv', nrows=10000000)"
      ],
      "metadata": {
        "id": "EDWlQyyqJ7Qu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "aW9Cv1n_J8uf",
        "outputId": "52719098-906c-41bb-af0a-4c89f3e057b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id  click      hour    C1  banner_pos   site_id site_domain  \\\n",
              "0  1.000009e+18      0  14102100  1005           0  1fbe01fe    f3845767   \n",
              "1  1.000017e+19      0  14102100  1005           0  1fbe01fe    f3845767   \n",
              "2  1.000037e+19      0  14102100  1005           0  1fbe01fe    f3845767   \n",
              "3  1.000064e+19      0  14102100  1005           0  1fbe01fe    f3845767   \n",
              "4  1.000068e+19      0  14102100  1005           1  fe8cc448    9166c161   \n",
              "\n",
              "  site_category    app_id app_domain  ... device_type device_conn_type    C14  \\\n",
              "0      28905ebd  ecad2386   7801e8d9  ...           1                2  15706   \n",
              "1      28905ebd  ecad2386   7801e8d9  ...           1                0  15704   \n",
              "2      28905ebd  ecad2386   7801e8d9  ...           1                0  15704   \n",
              "3      28905ebd  ecad2386   7801e8d9  ...           1                0  15706   \n",
              "4      0569f928  ecad2386   7801e8d9  ...           1                0  18993   \n",
              "\n",
              "   C15  C16   C17  C18  C19     C20  C21  \n",
              "0  320   50  1722    0   35      -1   79  \n",
              "1  320   50  1722    0   35  100084   79  \n",
              "2  320   50  1722    0   35  100084   79  \n",
              "3  320   50  1722    0   35  100084   79  \n",
              "4  320   50  2161    0   35      -1  157  \n",
              "\n",
              "[5 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-77f6d6ea-5c91-4293-9a9f-c2a2f8b72671\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>click</th>\n",
              "      <th>hour</th>\n",
              "      <th>C1</th>\n",
              "      <th>banner_pos</th>\n",
              "      <th>site_id</th>\n",
              "      <th>site_domain</th>\n",
              "      <th>site_category</th>\n",
              "      <th>app_id</th>\n",
              "      <th>app_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>device_type</th>\n",
              "      <th>device_conn_type</th>\n",
              "      <th>C14</th>\n",
              "      <th>C15</th>\n",
              "      <th>C16</th>\n",
              "      <th>C17</th>\n",
              "      <th>C18</th>\n",
              "      <th>C19</th>\n",
              "      <th>C20</th>\n",
              "      <th>C21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000009e+18</td>\n",
              "      <td>0</td>\n",
              "      <td>14102100</td>\n",
              "      <td>1005</td>\n",
              "      <td>0</td>\n",
              "      <td>1fbe01fe</td>\n",
              "      <td>f3845767</td>\n",
              "      <td>28905ebd</td>\n",
              "      <td>ecad2386</td>\n",
              "      <td>7801e8d9</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>15706</td>\n",
              "      <td>320</td>\n",
              "      <td>50</td>\n",
              "      <td>1722</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>-1</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000017e+19</td>\n",
              "      <td>0</td>\n",
              "      <td>14102100</td>\n",
              "      <td>1005</td>\n",
              "      <td>0</td>\n",
              "      <td>1fbe01fe</td>\n",
              "      <td>f3845767</td>\n",
              "      <td>28905ebd</td>\n",
              "      <td>ecad2386</td>\n",
              "      <td>7801e8d9</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15704</td>\n",
              "      <td>320</td>\n",
              "      <td>50</td>\n",
              "      <td>1722</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>100084</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000037e+19</td>\n",
              "      <td>0</td>\n",
              "      <td>14102100</td>\n",
              "      <td>1005</td>\n",
              "      <td>0</td>\n",
              "      <td>1fbe01fe</td>\n",
              "      <td>f3845767</td>\n",
              "      <td>28905ebd</td>\n",
              "      <td>ecad2386</td>\n",
              "      <td>7801e8d9</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15704</td>\n",
              "      <td>320</td>\n",
              "      <td>50</td>\n",
              "      <td>1722</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>100084</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.000064e+19</td>\n",
              "      <td>0</td>\n",
              "      <td>14102100</td>\n",
              "      <td>1005</td>\n",
              "      <td>0</td>\n",
              "      <td>1fbe01fe</td>\n",
              "      <td>f3845767</td>\n",
              "      <td>28905ebd</td>\n",
              "      <td>ecad2386</td>\n",
              "      <td>7801e8d9</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15706</td>\n",
              "      <td>320</td>\n",
              "      <td>50</td>\n",
              "      <td>1722</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>100084</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000068e+19</td>\n",
              "      <td>0</td>\n",
              "      <td>14102100</td>\n",
              "      <td>1005</td>\n",
              "      <td>1</td>\n",
              "      <td>fe8cc448</td>\n",
              "      <td>9166c161</td>\n",
              "      <td>0569f928</td>\n",
              "      <td>ecad2386</td>\n",
              "      <td>7801e8d9</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>18993</td>\n",
              "      <td>320</td>\n",
              "      <td>50</td>\n",
              "      <td>2161</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>-1</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77f6d6ea-5c91-4293-9a9f-c2a2f8b72671')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77f6d6ea-5c91-4293-9a9f-c2a2f8b72671 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77f6d6ea-5c91-4293-9a9f-c2a2f8b72671');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_features = ['site_id','site_domain','site_category','app_id','app_domain','app_category','device_id','device_ip','device_model' ]\n",
        "dense_features = ['id','hour','C1','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21']\n",
        "target = ['click']"
      ],
      "metadata": {
        "id": "d-Bg73htJ-hO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Label Encoding and normalization\n",
        "\n",
        "for feature in sparse_features:\n",
        "    label_encoding = LabelEncoder()\n",
        "    data[feature] = label_encoding.fit_transform(data[feature])\n",
        "mms = MinMaxScaler(feature_range=(0, 1))\n",
        "data[dense_features] = mms.fit_transform(data[dense_features])\n"
      ],
      "metadata": {
        "id": "EZNEIpLnKNvf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From: https://deepctr-doc.readthedocs.io/en/v0.8.5/Features.html\n",
        "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names \n",
        "\n",
        "all_feature_columns = [SparseFeat(feature, vocabulary_size=data[feature].max() + 1, embedding_dim=4) for i, feature in enumerate(sparse_features)] \\\n",
        "                         + [DenseFeat(feature, 1, ) for feature in dense_features]"
      ],
      "metadata": {
        "id": "SmmWvcnBK0Z4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_linear_columns = all_feature_columns\n",
        "linear_columns = all_feature_columns"
      ],
      "metadata": {
        "id": "dEGAIhMRLI4H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_of_features =  get_feature_names(linear_columns + non_linear_columns)"
      ],
      "metadata": {
        "id": "Hqp78sUTLV33"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_feature_columns, len(all_feature_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfoJD4FtLlWf",
        "outputId": "4882e38b-189d-4b6a-ad95-bdd281d64b58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([SparseFeat(name='site_id', vocabulary_size=3496, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b921277d0>, embedding_name='site_id', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='site_domain', vocabulary_size=4585, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b7f32c590>, embedding_name='site_domain', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='site_category', vocabulary_size=23, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b7e695410>, embedding_name='site_category', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='app_id', vocabulary_size=5469, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b7e15cad0>, embedding_name='app_id', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='app_domain', vocabulary_size=390, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b7d8b6050>, embedding_name='app_domain', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='app_category', vocabulary_size=33, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b7c1cda50>, embedding_name='app_category', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='device_id', vocabulary_size=786741, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b7bc8d110>, embedding_name='device_id', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='device_ip', vocabulary_size=2129662, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b73e4fdd0>, embedding_name='device_ip', group_name='default_group', trainable=True),\n",
              "  SparseFeat(name='device_model', vocabulary_size=6863, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x7f2b73880e10>, embedding_name='device_model', group_name='default_group', trainable=True),\n",
              "  DenseFeat(name='id', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='hour', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C1', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='device_type', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='device_conn_type', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C14', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C15', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C16', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C17', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C18', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C19', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C20', dimension=1, dtype='float32', transform_fn=None),\n",
              "  DenseFeat(name='C21', dimension=1, dtype='float32', transform_fn=None)],\n",
              " 22)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "train_input = [train_data[name] for name in name_of_features]\n",
        "test_input = [test_data[name] for name in name_of_features]"
      ],
      "metadata": {
        "id": "4Bv4iHKYLnYO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.backend import batch_dot\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.ops.init_ops import Zeros, Ones, Constant, TruncatedNormal, \\\n",
        "        glorot_normal_initializer as glorot_normal, \\\n",
        "        glorot_uniform_initializer as glorot_uniform\n",
        "except ImportError:\n",
        "    from tensorflow.python.ops.init_ops_v2 import Zeros, Ones, Constant, TruncatedNormal, glorot_normal, glorot_uniform\n",
        "\n",
        "from tensorflow.python.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "from tensorflow.python.layers import utils\n",
        "\n",
        "from deepctr.layers.activation import activation_layer\n",
        "from deepctr.layers.utils import concat_func, reduce_sum, softmax, reduce_mean\n",
        "\n",
        "try:\n",
        "    from tensorflow.python.keras.layers import BatchNormalization\n",
        "except ImportError:\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization"
      ],
      "metadata": {
        "id": "M5O2dqqTMMGP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAttentionMechanism(Layer):\n",
        "    # Initialasing multi head attention mechanism\n",
        "    def __init__(self, attention_embedding=8, number_of_heads=4, residual=True, use_scale =False, seed=1024, **kwargs):\n",
        "\n",
        "        self.attention_embedding = attention_embedding\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.residual = residual\n",
        "        self.seed = seed\n",
        "        self.use_scale = use_scale\n",
        "\n",
        "        super(MultiAttentionMechanism, self).__init__(**kwargs)\n",
        "\n",
        "    # Used to create weights that depend on the input shape\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        embedding_vector = int(input_shape[-1])\n",
        "        self.q = self.add_weight(name='query', shape=[embedding_vector, self.attention_embedding * self.number_of_heads],\n",
        "                                       dtype=tf.float32,\n",
        "                                       initializer=TruncatedNormal(seed=self.seed))\n",
        "        self.k = self.add_weight(name='key', shape=[embedding_vector, self.attention_embedding * self.number_of_heads],\n",
        "                                     dtype=tf.float32,\n",
        "                                     initializer=TruncatedNormal(seed=self.seed + 1))\n",
        "        self.v = self.add_weight(name='value', shape=[embedding_vector, self.attention_embedding * self.number_of_heads],\n",
        "                                       dtype=tf.float32,\n",
        "                                       initializer=TruncatedNormal(seed=self.seed + 2))\n",
        "        if self.residual:\n",
        "            self.r = self.add_weight(name='res', shape=[embedding_vector, self.attention_embedding * self.number_of_heads],\n",
        "                                         dtype=tf.float32,\n",
        "                                         initializer=TruncatedNormal(seed=self.seed))\n",
        "\n",
        "\n",
        "        super(MultiAttentionMechanism, self).build(input_shape)\n",
        "\n",
        "    # Performs the logic of applying the layer to the inputs\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        query = tf.tensordot(inputs, self.q,axes=(-1, 0))\n",
        "        key = tf.tensordot(inputs, self.k, axes=(-1, 0))\n",
        "        value = tf.tensordot(inputs, self.v, axes=(-1, 0))\n",
        "\n",
        "\n",
        "        query = tf.stack(tf.split(query, self.number_of_heads, axis=2))\n",
        "        key = tf.stack(tf.split(key, self.number_of_heads, axis=2))\n",
        "        value = tf.stack(tf.split(value, self.number_of_heads, axis=2))\n",
        "\n",
        "        dot_product = tf.matmul(query, key, transpose_b=True)\n",
        "        if self.use_scale:\n",
        "            dot_product /= self.attention_embedding ** 0.5\n",
        "\n",
        "        self.attention_output = softmax(dot_product)\n",
        "\n",
        "        output = tf.matmul(self.attention_output,value)\n",
        "        output = tf.concat(tf.split(output, self.number_of_heads, ), axis=-1)\n",
        "        output = tf.squeeze(output, axis=0)\n",
        "\n",
        "        if self.residual:\n",
        "            output += tf.tensordot(inputs, self.r, axes=(-1, 0))\n",
        "        output = tf.nn.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "        return (None, input_shape[1], self.attention_embedding * self.number_of_heads)\n",
        "\n",
        "    #  Returns a dictionary containing the configuration used to initialize this layer\n",
        "    def get_config(self, ):\n",
        "        config = {'attention_embedding': self.attention_embedding, 'number_of_heads': self.number_of_heads, 'residual': self.residual,\n",
        "                  'seed': self.seed}\n",
        "        base_config = super(MultiAttentionMechanism, self).get_config()\n",
        "        base_config.update(config)\n",
        "        return base_config\n",
        "\n",
        "class ElementWiseFM(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        super(ElementWiseFM, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_size):\n",
        "\n",
        "        super(ElementWiseFM, self).build(\n",
        "            input_size)  \n",
        "\n",
        "    def call(self, inputs_features, **kwargs):\n",
        "\n",
        "        combined_value = inputs_features\n",
        "        square_sum = tf.square(reduce_sum(\n",
        "            combined_value, axis=1, keep_dims=True))\n",
        "        sum_square = reduce_sum(\n",
        "            combined_value * combined_value, axis=1, keep_dims=True)\n",
        "        result = 0.5 * (square_sum - sum_square)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1, input_shape[-1])\n",
        "\n",
        "class DNN(Layer):\n",
        "\n",
        "    def __init__(self, hidden_layers, activation_function='relu', l2_regularizer=0, dropout=0, batch_norm=True, output_activation=None,\n",
        "                 seed=1024, **kwargs):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.activation_function = activation_function\n",
        "        self.l2_regularizer = l2_regularizer\n",
        "        self.dropout = dropout\n",
        "        self.batch_norm = batch_norm\n",
        "        self.output_activation = output_activation\n",
        "        self.seed = seed\n",
        "\n",
        "        super(DNN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        embedding_vector = input_shape[-1]\n",
        "        hidden_layers = [int(embedding_vector)] + list(self.hidden_layers)\n",
        "        self.kernel = [self.add_weight(name='kernel' + str(i),\n",
        "                                        shape=(\n",
        "                                            hidden_layers[i], hidden_layers[i + 1]),\n",
        "                                        initializer=glorot_normal(\n",
        "                                            seed=self.seed),\n",
        "                                        regularizer=l2(self.l2_regularizer),\n",
        "                                        trainable=True) for i in range(len(self.hidden_layers))]\n",
        "        self.bias = [self.add_weight(name='bias' + str(i),\n",
        "                                     shape=(self.hidden_layers[i],),\n",
        "                                     initializer=Zeros(),\n",
        "                                     trainable=True) for i in range(len(self.hidden_layers))]\n",
        "        if self.batch_norm:\n",
        "            self.batch_norm_layers = [BatchNormalization() for _ in range(len(self.hidden_layers))]\n",
        "\n",
        "        self.dropout_layers = [Dropout(self.dropout, seed=self.seed + i) for i in\n",
        "                               range(len(self.hidden_layers))]\n",
        "\n",
        "        self.activation_layers = [activation_layer(self.activation_function) for _ in range(len(self.hidden_layers))]\n",
        "\n",
        "        if self.output_activation:\n",
        "            self.activation_layers[-1] = activation_layer(self.output_activation)\n",
        "\n",
        "        super(DNN, self).build(input_shape)  \n",
        "\n",
        "    def call(self, inputs, training=None, **kwargs):\n",
        "\n",
        "        dnn_input = inputs\n",
        "\n",
        "        for i in range(len(self.hidden_layers)):\n",
        "            result = tf.nn.bias_add(tf.tensordot(\n",
        "                dnn_input, self.kernel[i], axes=(-1, 0)), self.bias[i])\n",
        "\n",
        "            if self.batch_norm:\n",
        "                result = self.batch_norm_layers[i](result, training=training)\n",
        "            try:\n",
        "                result = self.activation_layers[i](result, training=training)\n",
        "            except TypeError as e:  \n",
        "                print(\"make sure the activation function use training flag properly\", e)\n",
        "                result = self.activation_layers[i](result)\n",
        "\n",
        "            result = self.dropout_layers[i](result, training=training)\n",
        "            dnn_input = result\n",
        "\n",
        "        return dnn_input\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if len(self.hidden_layers) > 0:\n",
        "            shape = input_shape[:-1] + (self.hidden_layers[-1],)\n",
        "        else:\n",
        "            shape = input_shape\n",
        "\n",
        "        return tuple(shape)\n",
        "\n",
        "    def get_config(self, ):\n",
        "        configuration = {'activation_function': self.activation_function, 'hidden_layers': self.hidden_layers,\n",
        "                  'l2_regularizer': self.l2_regularizer, 'batch_norm': self.batch_norm, 'dropout': self.dropout,\n",
        "                  'output_activation': self.output_activation, 'seed': self.seed}\n",
        "        base_configuration = super(DNN, self).get_config()\n",
        "        return dict(list(base_configuration.items()) + list(configuration.items()))\n"
      ],
      "metadata": {
        "id": "F8mm_ZtmMQTn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Flatten, Concatenate, Dense\n",
        "from deepctr.layers.utils import concat_func, add_func, combined_dnn_input\n",
        "from deepctr.feature_column import build_input_features, get_linear_logit, input_from_feature_columns\n",
        "from deepctr.layers.core import PredictionLayer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def MANFM(linear_columns, non_linear_columns, number_attention_layer=4, attention_embedding_size=8, attention_head_number=4,\n",
        "            residual_con=True,\n",
        "            dnn_hidden_units=(300, 300, 300), dnn_activation='relu', l2_reg_linear=1e-5,\n",
        "            l2_reg_embedding=1e-5, l2_reg_dnn=0, dnn_use_bn=True, dnn_dropout=0, seed=1024,\n",
        "            task='binary', ):\n",
        "\n",
        "    #Returns the input features\n",
        "    features = build_input_features(non_linear_columns)\n",
        "    #The function returns a view object that shows a list of all the dictionary's values.\n",
        "    input = list(features.values())\n",
        "\n",
        "    # Logistic Regression Model\n",
        "    linear_output = get_linear_logit(features, linear_columns, seed=seed, prefix='linear', l2_reg=l2_reg_linear)\n",
        "\n",
        "    sparse_embedding_vectors, dense_value_vectors = input_from_feature_columns(features, non_linear_columns,l2_reg_embedding, seed)\n",
        "\n",
        "    # joins a collection of sparse vectors together and pass to att_input object.\n",
        "    att_input = concat_func(sparse_embedding_vectors, axis=1)\n",
        "\n",
        "    # For the number of attention layer in the model, do:\n",
        "    for i in range(number_attention_layer):\n",
        "        # the output of multi head attention fed to FM component.\n",
        "        attention_input = MultiAttentionMechanism(attention_embedding_size, attention_head_number, residual_con)(att_input)\n",
        "    # The last dimension of the output tensor is of size units instead of the same form as the inputs.\n",
        "    att_output = ElementWiseFM()(attention_input)\n",
        "    attention_output = Dense(1, use_bias=False)(att_output)\n",
        "\n",
        "    # sparse and dense vectors combined together\n",
        "    mlp_input = combined_dnn_input(sparse_embedding_vectors, dense_value_vectors)\n",
        "    # Vectors feed to hidden layers\n",
        "    mlp_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(mlp_input)\n",
        "    # The last dimension of the output tensor is of size units instead of the same form as the inputs.\n",
        "    mlp_output = Dense(1, use_bias=False)(mlp_output)\n",
        "\n",
        "    # The outcomes of the three components combined together.\n",
        "    final_output = add_func([linear_output, mlp_output, attention_output])\n",
        "    # Sigmoid function\n",
        "    output = PredictionLayer('binary')(final_output)\n",
        "\n",
        "    final_model = Model(inputs=input, outputs=output)\n",
        "\n",
        "    return final_model"
      ],
      "metadata": {
        "id": "h4NH9dzvNK13"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
        "\n",
        "model = MANFM(linear_columns, non_linear_columns, task='binary')\n",
        "model.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'], )\n",
        "\n",
        "history = model.fit(train_input, train_data[target].values,batch_size=256, epochs=10, verbose=2, validation_split=0.2, )\n",
        "prediction = model.predict(test_input, batch_size=256)\n",
        "print(\"test LogLoss\", round(log_loss(test_data[target].values, prediction), 4))\n",
        "print(\"test AUC\", round(roc_auc_score(test_data[target].values, prediction), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKHoKmIgNN0X",
        "outputId": "9dc28f1c-799f-431c-a30b-4d86217c4f17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 - 194s - loss: 0.3987 - binary_crossentropy: 0.3874 - val_loss: 0.3960 - val_binary_crossentropy: 0.3831\n",
            "Epoch 2/10\n",
            "25000/25000 - 191s - loss: 0.3946 - binary_crossentropy: 0.3802 - val_loss: 0.3949 - val_binary_crossentropy: 0.3809\n",
            "Epoch 3/10\n",
            "25000/25000 - 191s - loss: 0.3941 - binary_crossentropy: 0.3782 - val_loss: 0.3949 - val_binary_crossentropy: 0.3799\n",
            "Epoch 4/10\n",
            "25000/25000 - 190s - loss: 0.3935 - binary_crossentropy: 0.3766 - val_loss: 0.3955 - val_binary_crossentropy: 0.3797\n",
            "Epoch 5/10\n",
            "25000/25000 - 191s - loss: 0.3934 - binary_crossentropy: 0.3757 - val_loss: 0.3956 - val_binary_crossentropy: 0.3793\n",
            "Epoch 6/10\n",
            "25000/25000 - 190s - loss: 0.3933 - binary_crossentropy: 0.3750 - val_loss: 0.3954 - val_binary_crossentropy: 0.3786\n",
            "Epoch 7/10\n",
            "25000/25000 - 190s - loss: 0.3929 - binary_crossentropy: 0.3742 - val_loss: 0.3958 - val_binary_crossentropy: 0.3788\n",
            "Epoch 8/10\n",
            "25000/25000 - 191s - loss: 0.3927 - binary_crossentropy: 0.3738 - val_loss: 0.3959 - val_binary_crossentropy: 0.3785\n",
            "Epoch 9/10\n",
            "25000/25000 - 193s - loss: 0.3926 - binary_crossentropy: 0.3735 - val_loss: 0.3956 - val_binary_crossentropy: 0.3782\n",
            "Epoch 10/10\n",
            "25000/25000 - 190s - loss: 0.3925 - binary_crossentropy: 0.3731 - val_loss: 0.3961 - val_binary_crossentropy: 0.3785\n",
            "test LogLoss 0.3788\n",
            "test AUC 0.774\n"
          ]
        }
      ]
    }
  ]
}